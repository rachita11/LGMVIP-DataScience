{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01365d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cfd262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "783387c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape , X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "55e652c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x278425dbf70>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(221)\n",
    "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "493fc2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape to be [samples][width][height][channels]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a7e63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize inputs\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eebfbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e1f360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a one hot encoding of the class values, transforming the vector of class integers into a binary matrix.\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17580a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e207471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model has two main aspects:\n",
    "#the feature extraction front end comprised of convolutional and pooling layers, and the classifier backend that will make\n",
    "#a prediction.\n",
    "\n",
    "#For the convolutional front-end, we can start with a single convolutional layer with a small filter size (3,3)\n",
    "#and a modest number of filters (32) followed by a max pooling layer.\n",
    "#The filter maps can then be flattened to provide features to the classifier.\n",
    "\n",
    "#Given that the problem is a multi-class classification task,\n",
    "#we know that we will require an output layer with 10 nodes in order to predict the probability distribution of an image\n",
    "#belonging to each of the 10 classes. This will also require the use of a softmax activation function.\n",
    "#Between the feature extractor and the output layer, we can add a dense layer to interpret the features, in this case with \n",
    "#100 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cebbc515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca4169cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 94s 40ms/step - loss: 0.2169 - val_loss: 0.0507\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 72s 38ms/step - loss: 0.0370 - val_loss: 0.0353\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 73s 39ms/step - loss: 0.0203 - val_loss: 0.0391\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 71s 38ms/step - loss: 0.0136 - val_loss: 0.0394\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 69s 37ms/step - loss: 0.0102 - val_loss: 0.0446\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 72s 38ms/step - loss: 0.0083 - val_loss: 0.0446\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 69s 37ms/step - loss: 0.0032 - val_loss: 0.0427\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 68s 36ms/step - loss: 0.0052 - val_loss: 0.0469\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 68s 36ms/step - loss: 0.0056 - val_loss: 0.0452\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 70s 37ms/step - loss: 0.0031 - val_loss: 0.0580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x278449b2e20>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the model\n",
    "model = modelling()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec996ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Error: 0.06%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f55f82e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f38f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image\n",
    "img = load_img('example.png', color_mode = \"grayscale\", target_size=(28, 28))\n",
    "# convert to array\n",
    "img = img_to_array(img)\n",
    "# reshape into a single sample with 1 channel\n",
    "img = img.reshape(1, 28, 28, 1)\n",
    "# prepare pixel data\n",
    "img = img.astype('float32')\n",
    "img = img / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc36ada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5441bcc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit = np.argmax(model.predict(img), axis=-1)  \n",
    "digit[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53a276d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
